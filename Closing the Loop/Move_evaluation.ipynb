{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ce439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "userdata.get('gemini_key')\n",
    "API_KEY = userdata.get('gemini_key')\n",
    "genai.configure(api_key=API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23da4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(transcript_text: str) -> str:\n",
    "    return f\"\"\"\n",
    "\n",
    "You are an expert tutor reviewer.The data you are given are limited by the following:\n",
    "\n",
    "1.They are not diarized, You would have to know who the tutor is by looking at the utterance itself.\n",
    "2.Lack of the problem the student is working on. The evidence of a student making an error might be from the reaction of the tutor afterwards.\n",
    "\n",
    "Analyze the tutoring Audio transcripts below and score the tutor on the following 6 binary (0 or 1) dimensions.\n",
    "For each dimension, return both a binary score (0 or 1) and a short evidence from the transcript under that dimension name.\n",
    "\n",
    "RUBRIC DIMENSIONS (Return 0 or 1 for each)\n",
    "\n",
    "1. reacting_to_errors:\n",
    "    -0 = The student made a mistake and the tutor either gave the answer OR pointed out the error directly.\n",
    "    -1 = The tutor responded to a math error by asking the student to explain their thinking OR prompting them to think again.\n",
    "    -0 = The tutor does not react to any math error.\n",
    "\n",
    "\n",
    "2. giving_praise\n",
    "\n",
    "    -1 = Tutor Praises the student for their effort during the problem  (e.g.‚Äúgood job. now,what about this one?‚Äù, ‚ÄúI like how you kept working on that question‚Äù)\n",
    "    -0 = Tutor does not praise the student\n",
    "\n",
    "3. determining_what_students_know\n",
    "\n",
    "    -1 = Tutor asks open-ended questions to check what the student already knows on a problem (e.g., ‚ÄúHow would you start?‚Äù)\n",
    "    -0 = No attempt to check prior knowledge\n",
    "\n",
    "4. affirming_correct_attempt\n",
    "\n",
    "    -1 = Tutor affirms a correct response or student's correct reasoning (e.g., ‚ÄúYes, that‚Äôs right!‚Äù)\n",
    "    -0 = Tutor does not respond to a correct attempt to an explanation of the student's reasoning\n",
    "    -0 = Tutor does not respond to a correct answer\n",
    "\n",
    "5. asking_guiding_questions\n",
    "\n",
    "    -1 = Tutor asks guiding questions (e.g., ‚ÄúWhat‚Äôs the next step?‚Äù, ‚ÄúWhat are we trying to solve?‚Äù)\n",
    "    -0 = Tutor only gives instructions or answers without questions\n",
    "\n",
    "6. prompting_explanation\n",
    "\n",
    "    -1 = Tutor prompts student to explain their thinking (e.g., ‚ÄúCan you explain how you got that answer?‚Äù, \"do you want to talk through what you're thinking?\")\n",
    "    -0 = No prompt to explain reasoning\n",
    "\n",
    "Return your output STRICTLY in this JSON format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "\n",
    "  \"reacting_to_errors\": 0,\n",
    "  \"reacting_to_errors_evidence\": \"No evidence of a student making a math error\",\n",
    "  \"giving_praise\": 1,\n",
    "  \"giving_praise_evidence\": \"[03:00]: 'Nice job!'\",\n",
    "  \"determining_what_students_know\": 0,\n",
    "  \"determining_what_students_know_evidence\": \"No open-ended question to assess prior knowledge.\",\n",
    "  \"affirming_correct_attempt\": 1,\n",
    "  \"affirming_correct_attempt_evidence\": \"[05:01]: 'Yes, that‚Äôs exactly right.'\",\n",
    "  \"asking_guiding_questions\": 1,\n",
    "  \"asking_guiding_questions_evidence\": \"[06:12]: 'What should we do first here?'\",\n",
    "  \"prompting_explanation\": 0,\n",
    "  \"prompting_explanation_evidence\": \"No evidence the tutor asked the student to explain their thinking.\"\n",
    "\n",
    "}}\n",
    "Audio transcript:\n",
    "\\\"\\\"\\\"\n",
    "{transcript_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5962487",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \" \"\n",
    "OUTPUT_CSV = \"\"\n",
    "BATCH_SIZE = 20\n",
    "WAIT_TIME_BETWEEN_BATCHES = 5\n",
    "\n",
    "fieldnames = [\n",
    "    \"file\",\n",
    "    \"reacting_to_errors\",\n",
    "    \"giving_praise\",\n",
    "    \"determining_what_students_know\",\n",
    "    \"affirming_correct_attempt\",\n",
    "    \"asking_guiding_questions\",\n",
    "    \"prompting_explanation\",\n",
    "   \n",
    "\n",
    "]\n",
    "\n",
    "# Track already processed files\n",
    "completed_files = set()\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    df_existing = pd.read_csv(OUTPUT_CSV)\n",
    "    completed_files = set(df_existing[\"file\"].tolist())\n",
    "\n",
    "# Collect new .vtt files\n",
    "all_vtt_files = [\n",
    "    f for f in os.listdir(ROOT_DIR)\n",
    "    if f.endswith(\".vtt\") and f not in completed_files\n",
    "]\n",
    "\n",
    "def process_file(filename):\n",
    "    file_path = os.path.join(ROOT_DIR, filename)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            transcript_text = f.read()\n",
    "\n",
    "        prompt = build_prompt(transcript_text)\n",
    "        response = model.generate_content(prompt)\n",
    "        content = response.text.strip()\n",
    "\n",
    "        if content.startswith(\"```json\"):\n",
    "            content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "        scores = json.loads(content)\n",
    "\n",
    "    #    model output with evidence\n",
    "        print(f\"\\nüìò {filename} \\n{json.dumps(scores, indent=2)}\\n\")\n",
    "\n",
    "        \n",
    "        row = {\"file\": filename}\n",
    "        for dim in fieldnames[1:]:\n",
    "            row[dim] = scores.get(dim, 0)\n",
    "\n",
    "        return row\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "# batch processing\n",
    "for batch_start in range(0, len(all_vtt_files), BATCH_SIZE):\n",
    "    batch_files = all_vtt_files[batch_start: batch_start + BATCH_SIZE]\n",
    "    print(f\"\\n Processing batch {batch_start // BATCH_SIZE + 1} ({len(batch_files)} files)\")\n",
    "\n",
    "    batch_results = []\n",
    "    for filename in tqdm(batch_files):\n",
    "        result = process_file(filename)\n",
    "        if result:\n",
    "            batch_results.append(result)\n",
    "\n",
    "    # Save just the scores\n",
    "    if batch_results:\n",
    "        pd.DataFrame(batch_results).to_csv(\n",
    "            OUTPUT_CSV, mode=\"a\", header=not os.path.exists(OUTPUT_CSV), index=False\n",
    "        )\n",
    "        print(f\"üçÄBatch saved: {len(batch_results)} results\")\n",
    "    else:\n",
    "        print(\"No results in this batch (all failed?)\")\n",
    "\n",
    "    print(f\"Waiting {WAIT_TIME_BETWEEN_BATCHES} seconds before next batch...\")\n",
    "    time.sleep(WAIT_TIME_BETWEEN_BATCHES)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
