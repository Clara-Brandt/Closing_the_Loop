{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ce439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e04bcd",
   "metadata": {},
   "source": [
    "## Opportunity Detection Procedure\n",
    "\n",
    "Opportunity detection was performed using a single analysis pipeline.\n",
    "The same notebook was reused across tutor moves by substituting the\n",
    "corresponding opportunity prompt for each construct.\n",
    "\n",
    "For reproducibility, opportunity prompts were not executed jointly.\n",
    "Instead, each tutor move was evaluated separately by loading its\n",
    "construct-specific opportunity prompt and running the notebook end to\n",
    "end. This ensures that opportunity identification remains aligned with\n",
    "the theoretical definition of each tutor move.\n",
    "\n",
    "To reproduce this process, users should select the desired opportunity\n",
    "prompt from the prompts directory, update the prompt path in the\n",
    "configuration cell below, and rerun the notebook for that construct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are working from colab notebook, set your key in 'secrets' set name to 'gemini_key'\n",
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "userdata.get('gemini_key')\n",
    "API_KEY = userdata.get('gemini_key')\n",
    "genai.configure(api_key=API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "REPLACE WITH AN OPPORTUNITY PROMPT from prompts/LLM Implementation of Tutors Moves in Transcripts Scoring Prompts\n",
    "eg: REACT_ERRORS Opportunity Prompt\n",
    "\"\"\"\n",
    "def build_prompt(transcript_text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are an expert tutor reviewer.\n",
    "\n",
    "\n",
    "Your task is to determine whether the following tutoring transcript contains a clear OPPORTUNITY for a tutor to REACT TO A STUDENT'S MATH ERROR.\n",
    "---\n",
    "\n",
    "An OPPORTUNITY TO REACT TO A STUDENT'S MATH ERROR exists when:\n",
    "- A student makes a mistake while solving a math problem(ex: The student incorrectly solves 118 + 18 = 126)\n",
    "\n",
    "Scoring rules:\n",
    "-1: There was an opportunity for a tutor to react to a student's math error\n",
    "-0: There was no opportunity for a tutor to react to a student's math error\n",
    "\n",
    "If the student and the tutor did not work on any math problems then there was no opportunity.\n",
    "\n",
    "---\n",
    "Constraints:\n",
    "- The transcript is not diarized. You'll need to infer who is speaking based on the text itself.\n",
    "- Focus only on students making math errors, not whether the tutor reacted effectively or ineffectively.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Format your output in valid JSON using this format ONLY:\n",
    "\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"had_opportunity_to_react_to_errors\": 1,\n",
    "  \"evidence\": \"at [00:09], The student made a conceptual error, defining 'mean' as 'The one that comes up the most.\"\n",
    "}}\n",
    "\n",
    "\n",
    "Audio transcript:\n",
    "\\\"\\\"\\\"\n",
    "{transcript_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5962487",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"\"\n",
    "OUTPUT_CSV = \"\"\n",
    "BATCH_SIZE = 20\n",
    "WAIT_TIME_BETWEEN_BATCHES = 5 #To prevent Gemini from timing-out\n",
    "\n",
    "fieldnames = [\n",
    "    \"file\",\n",
    "    \"had_opportunity_to_react_to_errors\",\n",
    "    \"evidence\"\n",
    "]\n",
    "\n",
    "# Track already processed files\n",
    "completed_files = set()\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    df_existing = pd.read_csv(OUTPUT_CSV)\n",
    "    completed_files = set(df_existing[\"file\"].tolist())\n",
    "\n",
    "# Collect new .vtt files\n",
    "all_vtt_files = [\n",
    "    f for f in os.listdir(ROOT_DIR)\n",
    "    if f.endswith(\".vtt\") and f not in completed_files\n",
    "]\n",
    "\n",
    "def process_file(filename):\n",
    "    file_path = os.path.join(ROOT_DIR, filename)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            transcript_text = f.read()\n",
    "\n",
    "        prompt = build_prompt(transcript_text)\n",
    "        response = model.generate_content(prompt)\n",
    "        content = response.text.strip()\n",
    "\n",
    "        if content.startswith(\"```json\"):\n",
    "            content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "        scores = json.loads(content)\n",
    "\n",
    "    #    model output with evidence\n",
    "        print(f\"\\nüìò {filename} \\n{json.dumps(scores, indent=2)}\\n\")\n",
    "\n",
    "        \n",
    "        row = {\"file\": filename}\n",
    "        for dim in fieldnames[1:]:\n",
    "            row[dim] = scores.get(dim, 0)\n",
    "\n",
    "        return row\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "# batch processing\n",
    "for batch_start in range(0, len(all_vtt_files), BATCH_SIZE):\n",
    "    batch_files = all_vtt_files[batch_start: batch_start + BATCH_SIZE]\n",
    "    print(f\"\\n Processing batch {batch_start // BATCH_SIZE + 1} ({len(batch_files)} files)\")\n",
    "\n",
    "    batch_results = []\n",
    "    for filename in tqdm(batch_files):\n",
    "        result = process_file(filename)\n",
    "        if result:\n",
    "            batch_results.append(result)\n",
    "\n",
    "    # Save just the scores\n",
    "    if batch_results:\n",
    "        pd.DataFrame(batch_results).to_csv(\n",
    "            OUTPUT_CSV, mode=\"a\", header=not os.path.exists(OUTPUT_CSV), index=False\n",
    "        )\n",
    "        print(f\"üçÄBatch saved: {len(batch_results)} results\")\n",
    "    else:\n",
    "        print(\"No results in this batch (all failed?)\")\n",
    "\n",
    "    print(f\"Waiting {WAIT_TIME_BETWEEN_BATCHES} seconds before next batch...\")\n",
    "    time.sleep(WAIT_TIME_BETWEEN_BATCHES)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
